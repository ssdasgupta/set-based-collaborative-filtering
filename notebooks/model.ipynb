{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train] [--evaluate] [--predict]\n",
      "                             [--model_dir MODEL_DIR] [--model_name MODEL_NAME]\n",
      "                             [--train_data TRAIN_DATA] [--test_data TEST_DATA]\n",
      "                             [--user_data USER_DATA] [--item_data ITEM_DATA]\n",
      "                             [--user_attributes_data USER_ATTRIBUTES_DATA]\n",
      "                             [--item_attributes_data ITEM_ATTRIBUTES_DATA]\n",
      "                             [--n_epochs N_EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--lr LR] [--wd WD]\n",
      "                             [--embedding_dim EMBEDDING_DIM] [--seed SEED]\n",
      "                             [--verbose]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/ssdasgupta/Library/Jupyter/runtime/kernel-v2-913BGHljbuamyuX.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssdasgupta/miniconda3/envs/boxrec/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_users_attr,\n",
    "                 n_items_attr,\n",
    "                 embedding_dim=20):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(n_users, embedding_dim, sparse=True)\n",
    "        self.item_embeddings = nn.Embedding(n_items, embedding_dim, sparse=True)\n",
    "        self.user_attribute_embeddings = nn.Embedding(n_users_attr, embedding_dim, sparse=True)\n",
    "        self.item_attribute_embeddings = nn.Embedding(n_items_attr, embedding_dim, sparse=True)\n",
    "\n",
    "    def forward(self, user, item, user_attributes, item_attributes):\n",
    "        user_item_interaction = (self.user_embeddings(user) * self.item_embeddings(item)).sum(1)\n",
    "        user_attribute_user_interaction = (self.user_attribute_embeddings(user_attributes) * self.user_embeddings(user)).sum(1)\n",
    "        item_attribute_item_interaction = (self.item_attribute_embeddings(item_attributes) * self.item_embeddings(item)).sum(1)\n",
    "        return user_item_interaction + user_attribute_user_interaction + item_attribute_item_interaction\n",
    "\n",
    "    def predict(self, user, item, user_attributes, item_attributes):\n",
    "        return self.forward(user, item, user_attributes, item_attributes)\n",
    "\n",
    "\n",
    "class MatrixFactorizationWithBias(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_users_attr,\n",
    "                 n_items_attr,\n",
    "                 embedding_dim=20):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(n_users, embedding_dim, sparse=True)\n",
    "        self.item_embeddings = nn.Embedding(n_items, embedding_dim, sparse=True)\n",
    "        self.user_attribute_embeddings = nn.Embedding(n_users_attr, embedding_dim, sparse=True)\n",
    "        self.item_attribute_embeddings = nn.Embedding(n_items_attr, embedding_dim, sparse=True)\n",
    "        self.user_biases = nn.Embedding(n_users, 1, sparse=True)\n",
    "        self.item_biases = nn.Embedding(n_items, 1, sparse=True)\n",
    "        self.user_attribute_biases = nn.Embedding(n_users_attr, 1, sparse=True)\n",
    "        self.item_attribute_biases = nn.Embedding(n_items_attr, 1, sparse=True)\n",
    "\n",
    "    def forward(self, user, item, user_attributes, item_attributes):\n",
    "        user_item_interaction = (self.user_embeddings(user) * self.item_embeddings(item)).sum(1)\n",
    "        user_attribute_user_interaction = (self.user_attribute_embeddings(user_attributes) * self.user_embeddings(user)).sum(1)\n",
    "        item_attribute_item_interaction = (self.item_attribute_embeddings(item_attributes) * self.item_embeddings(item)).sum(1)\n",
    "        user_bias = self.user_biases(user).squeeze()\n",
    "        item_bias = self.item_biases(item).squeeze()\n",
    "        user_attribute_bias = self.user_attribute_biases(user_attributes).squeeze()\n",
    "        item_attribute_bias = self.item_attribute_biases(item_attributes).squeeze()\n",
    "        return user_item_interaction + user_attribute_user_interaction + item_attribute_item_interaction + user_bias + item_bias + user_attribute_bias + item_attribute_bias\n",
    "\n",
    "    def predict(self, user, item, user_attributes, item_attributes):\n",
    "        return self.forward(user, item, user_attributes, item_attributes)\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs, lr, wd, device, model_dir, model_name, verbose):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for i, (user, item, user_attributes, item_attributes, rating) in enumerate(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            user_attributes = user_attributes.to(device)\n",
    "            item_attributes = item_attributes.to(device)\n",
    "            rating = rating.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(user, item, user_attributes, item_attributes)\n",
    "            loss = criterion(outputs, rating)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_loss = 0.0\n",
    "        for i, (user, item, user_attributes, item_attributes, rating) in enumerate(test_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            user_attributes = user_attributes.to(device)\n",
    "            item_attributes = item_attributes.to(device)\n",
    "            rating = rating.to(device)\n",
    "            outputs = model(user, item, user_attributes, item_attributes)\n",
    "            loss = criterion(outputs, rating)\n",
    "            test_loss += loss.item()\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        if verbose:\n",
    "            print('epoch: {}, train loss: {}, test loss: {}'.format(epoch, train_loss, test_loss))\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, model_name))\n",
    "    return train_losses, test_losses\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (user, item, user_attributes, item_attributes, rating) in enumerate(test_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            user_attributes = user_attributes.to(device)\n",
    "            item_attributes = item_attributes.to(device)\n",
    "            rating = rating.to(device)\n",
    "            outputs = model(user, item, user_attributes, item_attributes)\n",
    "            loss = criterion(outputs, rating)\n",
    "            test_loss += loss.item()\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    return test_loss\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='train the model')\n",
    "    parser.add_argument('--evaluate', action='store_true', help='evaluate the model')\n",
    "    parser.add_argument('--predict', action='store_true', help='predict the model')\n",
    "    parser.add_argument('--model_dir', type=str, default='model', help='model directory path')\n",
    "    parser.add_argument('--model_name', type=str, default='model.pth', help='model name')\n",
    "    parser.add_argument('--train_data', type=str, default='data/train.csv', help='train data path')\n",
    "    parser.add_argument('--test_data', type=str, default='data/test.csv', help='test data path')\n",
    "    parser.add_argument('--user_data', type=str, default='data/user.csv', help='user data path')\n",
    "    parser.add_argument('--item_data', type=str, default='data/item.csv', help='item data path')\n",
    "    parser.add_argument('--user_attributes_data', type=str, default='data/user_attributes.csv', help='user attributes data path')\n",
    "    parser.add_argument('--item_attributes_data', type=str, default='data/item_attributes.csv', help='item attributes data path')\n",
    "    parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024, help='batch size')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, help='learning rate')\n",
    "    parser.add_argument('--wd', type=float, default=0.0, help='weight decay')\n",
    "    parser.add_argument('--embedding_dim', type=int, default=20, help='embedding dimension')\n",
    "    parser.add_argument('--seed', type=int, default=1234, help='random seed')\n",
    "    parser.add_argument('--verbose', action='store_true', help='verbose')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('Loading data...')\n",
    "    train_df = pd.read_csv(args.train_data)\n",
    "    test_df = pd.read_csv(args.test_data)\n",
    "    user_df = pd.read_csv(args.user_data)\n",
    "    item_df = pd.read_csv(args.item_data)\n",
    "    user_attributes_df = pd.read_csv(args.user_attributes_data)\n",
    "    item_attributes_df = pd.read_csv(args.item_attributes_data)\n",
    "\n",
    "    print('Preprocessing data...')\n",
    "    user_ids = sorted(list(set(train_df['user_id'].unique().tolist() + test_df['user_id'].unique().tolist())))\n",
    "    user2id = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_ids = sorted(list(set(train_df['item_id'].unique().tolist() + test_df['item_id'].unique().tolist())))\n",
    "    item2id = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    user_attribute_ids = sorted(list(set(user_attributes_df['user_attribute'].unique().tolist())))\n",
    "    user_attribute2id = {user_attribute: i for i, user_attribute in enumerate(user_attribute_ids)}\n",
    "    item_attribute_ids = sorted(list(set(item_attributes_df['item_attribute'].unique().tolist())))\n",
    "    item_attribute2id = {item_attribute: i for i, item_attribute in enumerate(item_attribute_ids)}\n",
    "    train_df['user_id'] = train_df['user_id'].map(user2id)\n",
    "    train_df['item_id'] = train_df['item_id'].map(item2id)\n",
    "    test_df['user_id'] = test_df['user_id'].map(user2id)\n",
    "    test_df['item_id'] = test_df['item_id'].map(item2id)\n",
    "    user_attributes_df['user_attribute'] = user_attributes_df['user_attribute'].map(user_attribute2id)\n",
    "    item_attributes_df['item_attribute'] = item_attributes_df['item_attribute'].map(item_attribute2id)\n",
    "    train_df = train_df.sample(frac=1, random_state=args.seed)\n",
    "    test_df = test_df.sample(frac=1, random_state=args.seed)\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    user_attributes_df = user_attributes_df.sample(frac=1, random_state=args.seed)\n",
    "    item_attributes_df = item_attributes_df.sample(frac=1, random_state=args.seed)\n",
    "    user_attributes_df.reset_index(drop=True, inplace=True)\n",
    "    item_attributes_df.reset_index(drop=True, inplace=True)\n",
    "    train_user_ids = train_df['user_id'].values\n",
    "    train_item_ids = train_df['item_id'].values\n",
    "    train_user_attributes = user_attributes_df['user_attribute'].values\n",
    "    train_item_attributes = item_attributes_df['item_attribute'].values\n",
    "    train_ratings = train_df['rating'].values\n",
    "    test_user_ids = test_df['user_id'].values\n",
    "    test_item_ids = test_df['item_id'].values\n",
    "    test_user_attributes = user_attributes_df['user_attribute'].values\n",
    "    test_item_attributes = item_attributes_df['item_attribute'].values\n",
    "    test_ratings = test_df['rating'].values\n",
    "    train_dataset = TensorDataset(torch.LongTensor(train_user_ids),\n",
    "                                  torch.LongTensor(train_item_ids),\n",
    "                                  torch.LongTensor(train_user_attributes),\n",
    "                                  torch.LongTensor(train_item_attributes),\n",
    "                                  torch.FloatTensor(train_ratings))\n",
    "    test_dataset = TensorDataset(torch.LongTensor(test_user_ids),\n",
    "                                    torch.LongTensor(test_item_ids),\n",
    "                                    torch.LongTensor(test_user_attributes),\n",
    "                                    torch.LongTensor(test_item_attributes),\n",
    "                                    torch.FloatTensor(test_ratings))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    print('Building model...')\n",
    "    model = MatrixFactorizationWithBias(n_users=len(user_ids),\n",
    "                                        n_items=len(item_ids),\n",
    "                                        n_users_attr=len(user_attribute_ids),\n",
    "                                        n_items_attr=len(item_attribute_ids),\n",
    "                                        embedding_dim=args.embedding_dim)\n",
    "    \n",
    "    if args.train:\n",
    "        print('Training model...')\n",
    "        train_losses, test_losses = train(model, train_loader, test_loader, args.n_epochs, args.lr, args.wd, device, args.model_dir, args.model_name, args.verbose)\n",
    "        if args.verbose:\n",
    "            print('Saving losses...')\n",
    "        with open(os.path.join(args.model_dir, 'train_losses.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_losses, f)\n",
    "        with open(os.path.join(args.model_dir, 'test_losses.pkl'), 'wb') as f:\n",
    "            pickle.dump(test_losses, f)\n",
    "    if args.evaluate:\n",
    "        print('Evaluating model...')\n",
    "        model.load_state_dict(torch.load(os.path.join(args.model_dir, args.model_name)))\n",
    "        test_loss = evaluate(model, test_loader, device)\n",
    "        print('test loss: {}'.format(test_loss))\n",
    "    if args.predict:\n",
    "        print('Predicting model...')\n",
    "        model.load_state_dict(torch.load(os.path.join(args.model_dir, args.model_name)))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (user, item, user_attributes, item_attributes, rating) in enumerate(test_loader):\n",
    "                user = user.to(device)\n",
    "                item = item.to(device)\n",
    "                user_attributes = user_attributes.to(device)\n",
    "                item_attributes = item_attributes.to(device)\n",
    "                rating = rating.to(device)\n",
    "                outputs = model(user, item, user_attributes, item_attributes)\n",
    "                print('user: {}, item: {}, user_attributes: {}, item_attributes: {}, rating: {}, predicted rating: {}'.format(user, item, user_attributes, item_attributes, rating, outputs))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
