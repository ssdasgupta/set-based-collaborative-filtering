{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ml-latest-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/ratings.csv')\n",
    "movies = pd.read_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter interaction, and re-id the items and users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Filter. Design choice count < 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_count = {}\n",
    "for userID, movieID, rating in zip(ratings['userId'], ratings['movieId'], ratings['rating']):\n",
    "    user_id_count[userID] = user_id_count.get(userID, 0) + 1\n",
    "user_reject_list = [userID for userID, count in user_id_count.items() if count < 20]\n",
    "ratings = ratings[~ratings['userId'].isin(user_reject_list)]\n",
    "\n",
    "movie_id_count = {}\n",
    "for userID, movieID, rating in zip(ratings['userId'], ratings['movieId'], ratings['rating']):\n",
    "    movie_id_count[movieID] = movie_id_count.get(movieID, 0) + 1\n",
    "\n",
    "count_10_movies = [movieID for movieID, count in movie_id_count.items() if count < 10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_10_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create User dict and item dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx = {userID: i for i, userID in enumerate(ratings['userId'].unique())}\n",
    "movie2idx = {movieID: i for i, movieID in enumerate(ratings['movieId'].unique())}\n",
    "idx2movie = {i: movieID for i, movieID in enumerate(ratings['movieId'].unique())}\n",
    "ratings['userId'] = ratings['userId'].map(user2idx)\n",
    "ratings['movieId'] = ratings['movieId'].map(movie2idx)\n",
    "count_10_movies = [movie2idx[movieID] for movieID in count_10_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(movies.movieId)) - len(set(ratings.movieId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 671\n",
      "Number of movies: 9066\n",
      "Number of ratings: 100004\n"
     ]
    }
   ],
   "source": [
    "print('Number of users:', len(user2idx))\n",
    "print('Number of movies:', len(movie2idx))\n",
    "print('Number of ratings:', len(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the interaction in the form of an adjacency for leave-k-out\n",
    "Design choice: we are recording implicit feedbacks here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_rating_matrix_dict = {}\n",
    "movieid_set = set()\n",
    "for userID, movieID, rating in zip(ratings['userId'], ratings['movieId'], ratings['rating']):\n",
    "    if userID not in implicit_rating_matrix_dict:\n",
    "        implicit_rating_matrix_dict[userID] = [movieID]\n",
    "    else:\n",
    "        implicit_rating_matrix_dict[userID].append(movieID)\n",
    "    movieid_set.add(movieID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 671\n",
      "Number of movies: 9066\n"
     ]
    }
   ],
   "source": [
    "print('Number of users:', len(implicit_rating_matrix_dict))\n",
    "print('Number of movies:', len(movieid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a leave-k-out style split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_n_percent_out(data, n=10):\n",
    "    user_list = []\n",
    "    item_train_list = []\n",
    "    item_val_list = []\n",
    "    item_test_list = []\n",
    "    item_gt_list = []\n",
    "    movie_set = set()\n",
    "    movie_set_indv = set()\n",
    "    for user, item_list in data.items():\n",
    "        item_gt_list.append(item_list)\n",
    "        k = len(item_list) * n // 100\n",
    "        if k == 0:\n",
    "            print('user:', user, 'has less than 20 items')\n",
    "        # randomly select 2k items from the item list\n",
    "        val_test_items = np.random.choice(np.setdiff1d(item_list, count_10_movies), size=2*k, replace=False)\n",
    "        val_items = list(val_test_items[:k])\n",
    "        test_items = list(val_test_items[k:])\n",
    "        train_items = list(np.setdiff1d(item_list, val_test_items))\n",
    "        user_list.append(user)\n",
    "        item_train_list.append(train_items)\n",
    "        item_val_list.append(val_items)\n",
    "        item_test_list.append(test_items)\n",
    "        for item in item_list:\n",
    "            movie_set.add(item) \n",
    "        for item in train_items:\n",
    "            movie_set_indv.add(item)\n",
    "        # for item in val_items:\n",
    "        #     movie_set_indv.add(item)\n",
    "        # for item in test_items:\n",
    "        #     movie_set_indv.add(item)\n",
    "    print('number of items:', len(movie_set)) \n",
    "    print('number of items in total:', len(movie_set_indv))\n",
    "\n",
    "    return user_list, item_gt_list, item_train_list, item_val_list, item_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset splits\n",
    "- design choice:  90% 5% 5% randomly selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 9066\n",
      "number of items in total: 9066\n"
     ]
    }
   ],
   "source": [
    "user_list, item_gt_list, item_train_list, item_val_list, item_test_list = leave_n_percent_out(implicit_rating_matrix_dict, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test + train + val = all items\n",
    "for i in range(len(user_list)):\n",
    "    assert set(item_train_list[i] + item_val_list[i] + item_test_list[i]) == set(implicit_rating_matrix_dict[user_list[i]])\n",
    "\n",
    "# no overlap between train, val and test\n",
    "for i in range(len(user_list)):\n",
    "    assert len(np.intersect1d(item_train_list[i], item_val_list[i])) == 0\n",
    "    assert len(np.intersect1d(item_train_list[i], item_test_list[i])) == 0\n",
    "    assert len(np.intersect1d(item_val_list[i], item_test_list[i])) == 0\n",
    "\n",
    "# no empty train, val and test\n",
    "for i in range(len(user_list)):\n",
    "    assert len(item_train_list[i]) > 0\n",
    "    assert len(item_val_list[i]) > 0\n",
    "    assert len(item_test_list[i]) > 0\n",
    "\n",
    "# train set contains all items\n",
    "train_item_set = set()\n",
    "for i in range(len(user_list)):\n",
    "    for item in item_train_list[i]:\n",
    "        train_item_set.add(item)\n",
    "\n",
    "test_item_set = set()\n",
    "for i in range(len(user_list)):\n",
    "    for item in item_test_list[i]:\n",
    "        test_item_set.add(item)\n",
    "\n",
    "val_item_set = set()\n",
    "for i in range(len(user_list)):\n",
    "    for item in item_val_list[i]:\n",
    "        val_item_set.add(item)\n",
    "assert train_item_set.union(test_item_set).union(val_item_set) == set(movie2idx.values())\n",
    "\n",
    "# no overlap between train, val and test\n",
    "for i in range(len(user_list)):\n",
    "    for item in item_train_list[i]:\n",
    "        assert item not in item_val_list[i]\n",
    "        assert item not in item_test_list[i]\n",
    "    for item in item_val_list[i]:\n",
    "        assert item not in item_train_list[i]\n",
    "        assert item not in item_test_list[i]\n",
    "    for item in item_test_list[i]:\n",
    "        assert item not in item_train_list[i]\n",
    "        assert item not in item_val_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DF out of the adjencency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency2df(user_list, item_list_of_list):\n",
    "    user = []\n",
    "    item = []\n",
    "    for i in range(len(user_list)):\n",
    "        user.extend([user_list[i]] * len(item_list_of_list[i]))\n",
    "        item.extend(item_list_of_list[i])\n",
    "    return pd.DataFrame({'userId': user, 'movieId': item})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print/ Save test train val and id dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = adjacency2df(user_list, item_train_list)\n",
    "val_df = adjacency2df(user_list, item_val_list)\n",
    "test_df = adjacency2df(user_list, item_test_list)\n",
    "gt_df = adjacency2df(user_list, item_gt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/gt.csv', index=False)\n",
    "train_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/train.csv', index=False)\n",
    "val_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/val.csv', index=False)\n",
    "test_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2id_df = pd.DataFrame({'original_userId': list(user2idx.keys()), 'userId': list(user2idx.values())})\n",
    "movie2id_df = pd.DataFrame({'original_movieId': list(movie2idx.keys()), 'movieId': list(movie2idx.values())})\n",
    "user2id_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/user2id.csv', index=False)\n",
    "movie2id_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/movie2id.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre integration with dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the genre2movies\n",
    "- Generate statistics of what movies are in genre2movies \n",
    "- What movies are in movielens catelog.\n",
    "- What movies are in the movielens ratings (not every movie is in the catelog.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre2movies = pd.read_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/genre2movies/genre2movies.csv')\n",
    "movie_name2lensid = pd.read_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/ml-20m/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of movies in movie lens catelog:', len(set(movies.title)))\n",
    "print('Number of movies in genre2movies:', len(genre2movies.movie.unique()))\n",
    "print('Number of movies in movielens ratings', len(set(ratings.movieId)))\n",
    "print('Number of movies in genre2movies and in movielense catelog', len(set(movies.title).intersection(set(genre2movies.movie.unique()))))\n",
    "print('Number of movies in genre2movies and not in movielense catelog', len(set(genre2movies.movie.unique()) - set(movies.title)))\n",
    "print('Number of movies in movielense catelog and not in genre2movies', len(set(movies.title).difference(genre2movies.movie.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for name, id in zip(movies.title, movies.movieId):\n",
    "    if id not in movie2idx:\n",
    "        if name in genre2movies.movie.unique():\n",
    "            count += 1\n",
    "print('Number of movies in genre2movies and not in ratings', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the movies from the genre2movies to the movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name2id = {}\n",
    "count = 0\n",
    "for movie, id in zip(movie_name2lensid.title, movie_name2lensid.movieId):\n",
    "    if id not in movie2idx:\n",
    "        count+=1\n",
    "        continue\n",
    "    movie_name2id[movie] = movie2idx[id]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre2movieid = {}\n",
    "movie_not_in_dataset = set()\n",
    "movie_in_dataset = set()\n",
    "for movie, genre in zip(genre2movies.movie, genre2movies.genre):\n",
    "    if movie in movie_name2id:\n",
    "        if genre not in genre2movieid:\n",
    "            genre2movieid[genre] = [movie_name2id[movie]]\n",
    "        else:\n",
    "            genre2movieid[genre].append(movie_name2id[movie])\n",
    "        movie_in_dataset.add(movie)\n",
    "    else:\n",
    "        movie_not_in_dataset.add(movie)\n",
    "\n",
    "print(f\"# {len(movie_not_in_dataset)} Movie not found in the dataset:\")\n",
    "print(f\"# {len(movie_in_dataset)} Movie found in the dataset:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Genre, user to movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieid2genre = {}\n",
    "for genre, movieid_list in genre2movieid.items():\n",
    "    for movieid in movieid_list:\n",
    "        if movieid not in movieid2genre:\n",
    "            movieid2genre[movieid] = [genre]\n",
    "        else:\n",
    "            movieid2genre[movieid].append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_user2movieid = {}\n",
    "movies_not_found = set()\n",
    "movies_found = set()\n",
    "total_movies = set()\n",
    "for user, item_list in tqdm(implicit_rating_matrix_dict.items()):\n",
    "    total_movies.update(item_list)\n",
    "    for item in item_list:\n",
    "        if item in movieid2genre:\n",
    "            genre_list = movieid2genre[item]\n",
    "            for genre in genre_list:\n",
    "                if (genre, user) not in genre_user2movieid:\n",
    "                    genre_user2movieid[(genre, user)] = {item}\n",
    "                else:\n",
    "                    genre_user2movieid[(genre, user)].add(item)\n",
    "            movies_found.add(item)\n",
    "        else:\n",
    "            movies_not_found.add(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desgin Choice: only frequency greater than 20 considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the (genre, movies) pair that has less than 20 movies\n",
    "genre_user2movieid_filtered = {}\n",
    "for (genre, user), movieid_set in genre_user2movieid.items():\n",
    "    if len(movieid_set) >= 20:\n",
    "        genre_user2movieid_filtered[(genre, user)] = movieid_set\n",
    "print(f\"Number of (genre, user) tuple to evaluate on : {len(genre_user2movieid_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the genre, user, item dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = []\n",
    "genre_list = []\n",
    "item_list = []\n",
    "for (genre, user), items in genre_user2movieid_filtered.items():\n",
    "    user_list.extend([user] * len(items))\n",
    "    genre_list.extend([genre] * len(items))\n",
    "    item_list.extend(list(items))\n",
    "genre_user_item_df = pd.DataFrame({'userId': user_list, 'genre': genre_list, 'movieId': item_list})\n",
    "genre_user_item_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/genre_user_item.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tag data from Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag2movieid(tags_df):\n",
    "    tag2movieid = {}\n",
    "    for movieid, tag in zip(tags_df.movieId, tags_df.tag):\n",
    "        tag = str(tag).lower()\n",
    "        if movieid in movie2idx:\n",
    "            if tag in tag2movieid:\n",
    "                tag2movieid[tag].add(movie2idx[movieid])\n",
    "            else:\n",
    "                tag2movieid[tag] = {movie2idx[movieid]}\n",
    "    return tag2movieid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2movieid = get_tag2movieid(tags_df)\n",
    "tag_vocab = tag2movieid.keys()\n",
    "tag2id = {tag: i for i, tag in enumerate(tag_vocab)}\n",
    "tagid2movieid = {tag2id[tag]: list(movieid_set) for tag, movieid_set in tag2movieid.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2df(_dict, key_name='key', value_name='value'):\n",
    "    key_list = []\n",
    "    value_list = []\n",
    "    for key, value in _dict.items():\n",
    "        key_list.extend([key] * len(value))\n",
    "        value_list.extend(list(value))\n",
    "    return pd.DataFrame({key_name: key_list, value_name: value_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagid2movieid_df = dict2df(tagid2movieid, key_name='tagId', value_name='movieId')\n",
    "tag2id_df = pd.DataFrame({'tag': list(tag2id.keys()), 'tagId': list(tag2id.values())})\n",
    "tag2id_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/tag2id.csv', index=False)\n",
    "tagid2movieid_df.to_csv(f'/Users/ssdasgupta/research/set-based-collaborative-filtering/data/{dataset_name}/tag2movie.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-1M data processing 101 type eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/g_pstsjd04lcwnwzz8vkrdqm0000gq/T/ipykernel_93998/572840924.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(ratings, delimiter=\"::\", header=None, names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'method' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/ml-1m/ratings.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read the .dat file into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ratings, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovieId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])               \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print the DataFrame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/miniconda3/envs/boxrec/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/boxrec/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/boxrec/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/boxrec/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/boxrec/lib/python3.11/site-packages/pandas/io/common.py:719\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    716\u001b[0m errors \u001b[38;5;241m=\u001b[39m errors \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# read_csv does not know whether the buffer is opened in binary/text mode\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_binary_mode(path_or_buf, mode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    720\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# validate encoding and errors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxrec/lib/python3.11/site-packages/pandas/io/common.py:1181\u001b[0m, in \u001b[0;36m_is_binary_mode\u001b[0;34m(handle, mode)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(handle), text_classes):\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, _get_binary_io_classes()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m   1182\u001b[0m     handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\n\u001b[1;32m   1183\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'method' is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./data/ml-1m/ratings.dat\"\n",
    "\n",
    "# Read the .dat file into a DataFrame\n",
    "df = pd.read_csv(ratings, delimiter=\"::\", header=None, names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])               \n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "##read dat file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some dummy codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_data(data_path, batch_size):\n",
    "    data = pd.read_csv(data_path)\n",
    "    user = data['user_id'].values\n",
    "    item = data['item_id'].values\n",
    "    dataset = TensorDataset(torch.LongTensor(user), torch.LongTensor(item))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = load_data('/Users/ssdasgupta/research/set-based-collaborative-filtering/data/ml-latest-small/train.csv', 32)\n",
    "val_loader = load_data('/Users/ssdasgupta/research/set-based-collaborative-filtering/data/ml-latest-small/val.csv', 32)\n",
    "test_loader = load_data('/Users/ssdasgupta/research/set-based-collaborative-filtering/data/ml-latest-small/test.csv', 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    user = batch[0]\n",
    "    item = batch[1]\n",
    "    print(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boxrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
